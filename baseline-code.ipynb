{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import requirements","metadata":{}},{"cell_type":"code","source":"import os\nimport pdb\nimport wandb\nimport argparse\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom collections import defaultdict\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport numpy as np\nfrom tqdm import tqdm, trange\n\nfrom transformers import (\n    BertForSequenceClassification,\n    BertTokenizer,\n    AutoConfig,\n    AdamW\n)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:30:18.886064Z","iopub.execute_input":"2021-10-28T13:30:18.886396Z","iopub.status.idle":"2021-10-28T13:30:25.376698Z","shell.execute_reply.started":"2021-10-28T13:30:18.886313Z","shell.execute_reply":"2021-10-28T13:30:25.375868Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1. Preprocess","metadata":{}},{"cell_type":"code","source":"def make_id_file(task, tokenizer):\n    def make_data_strings(file_name):\n        data_strings = []\n        with open(os.path.join('../input/goormtextclassificationproject', file_name), 'r', encoding='utf-8') as f:\n            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]\n        for item in id_file_data:\n            data_strings.append(' '.join([str(k) for k in item]))\n        return data_strings\n        \n    print('it will take some times...')\n    train_pos = make_data_strings('sentiment.train.1')\n    train_neg = make_data_strings('sentiment.train.0')\n    dev_pos = make_data_strings('sentiment.dev.1')\n    dev_neg = make_data_strings('sentiment.dev.0')\n\n    print('make id file finished!')\n    return train_pos, train_neg, dev_pos, dev_neg","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:31:03.986627Z","iopub.execute_input":"2021-10-28T13:31:03.987256Z","iopub.status.idle":"2021-10-28T13:31:03.994288Z","shell.execute_reply.started":"2021-10-28T13:31:03.987218Z","shell.execute_reply":"2021-10-28T13:31:03.993360Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:31:05.798404Z","iopub.execute_input":"2021-10-28T13:31:05.799074Z","iopub.status.idle":"2021-10-28T13:31:10.391550Z","shell.execute_reply.started":"2021-10-28T13:31:05.799032Z","shell.execute_reply":"2021-10-28T13:31:10.390887Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:31:12.742026Z","iopub.execute_input":"2021-10-28T13:31:12.742856Z","iopub.status.idle":"2021-10-28T13:33:53.342843Z","shell.execute_reply.started":"2021-10-28T13:31:12.742818Z","shell.execute_reply":"2021-10-28T13:33:53.342077Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_pos[:10]","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:34:56.393829Z","iopub.execute_input":"2021-10-28T13:34:56.394111Z","iopub.status.idle":"2021-10-28T13:34:56.401067Z","shell.execute_reply.started":"2021-10-28T13:34:56.394077Z","shell.execute_reply":"2021-10-28T13:34:56.400173Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# datasets 라이브러리\nclass SentimentDataset(object):\n    def __init__(self, tokenizer, pos, neg):\n        self.tokenizer = tokenizer\n        self.data = []\n        self.label = []\n\n        for pos_sent in pos:\n            self.data += [self._cast_to_int(pos_sent.strip().split())]\n            self.label += [[1]]\n        for neg_sent in neg:\n            self.data += [self._cast_to_int(neg_sent.strip().split())]\n            self.label += [[0]]\n\n    def _cast_to_int(self, sample):\n        return [int(word_id) for word_id in sample]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sample = self.data[index]\n        return np.array(sample), np.array(self.label[index])","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:34:58.247713Z","iopub.execute_input":"2021-10-28T13:34:58.248464Z","iopub.status.idle":"2021-10-28T13:34:58.256027Z","shell.execute_reply.started":"2021-10-28T13:34:58.248430Z","shell.execute_reply":"2021-10-28T13:34:58.255075Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\ndev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:00.735634Z","iopub.execute_input":"2021-10-28T13:35:00.736183Z","iopub.status.idle":"2021-10-28T13:35:03.604051Z","shell.execute_reply.started":"2021-10-28T13:35:00.736146Z","shell.execute_reply":"2021-10-28T13:35:03.603298Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"for i, item in enumerate(train_dataset):\n    print(item)\n    if i == 10:\n        break","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:06.776689Z","iopub.execute_input":"2021-10-28T13:35:06.777242Z","iopub.status.idle":"2021-10-28T13:35:06.788077Z","shell.execute_reply.started":"2021-10-28T13:35:06.777208Z","shell.execute_reply":"2021-10-28T13:35:06.787230Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def collate_fn_sentiment(samples):\n    input_ids, labels = zip(*samples)\n    max_len = max(len(input_id) for input_id in input_ids)\n    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n\n    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n                             batch_first=True)\n    attention_mask = torch.tensor(\n        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n         sorted_indices])\n    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n\n    return input_ids, attention_mask, token_type_ids, position_ids, labels","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:10.159131Z","iopub.execute_input":"2021-10-28T13:35:10.159382Z","iopub.status.idle":"2021-10-28T13:35:10.167436Z","shell.execute_reply.started":"2021-10-28T13:35:10.159352Z","shell.execute_reply":"2021-10-28T13:35:10.166765Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# train_batch_size=32\n# eval_batch_size=32\n\ntrain_batch_size=64\neval_batch_size=64\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset,\n                                           batch_size=train_batch_size,\n                                           shuffle=True, collate_fn=collate_fn_sentiment,\n                                           pin_memory=True, num_workers=4)\ndev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n                                         shuffle=False, collate_fn=collate_fn_sentiment,\n                                         num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:12.767078Z","iopub.execute_input":"2021-10-28T13:35:12.767330Z","iopub.status.idle":"2021-10-28T13:35:12.779384Z","shell.execute_reply.started":"2021-10-28T13:35:12.767301Z","shell.execute_reply":"2021-10-28T13:35:12.778502Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# random seed\nrandom_seed=42\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased')\nmodel.to(device)","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-10-28T13:35:17.167231Z","iopub.execute_input":"2021-10-28T13:35:17.167494Z","iopub.status.idle":"2021-10-28T13:35:38.822379Z","shell.execute_reply.started":"2021-10-28T13:35:17.167464Z","shell.execute_reply":"2021-10-28T13:35:38.821688Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model.train()\n# learning_rate = 1e-6\nlearning_rate = 5e-5\noptimizer = AdamW(model.parameters(), lr=learning_rate)\n# t_total = len(train_loader) * train_epoch\n# scheduler = get_linear_schedule_with_warmup(optimizer, t_toal / 10, t_total)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:38.823798Z","iopub.execute_input":"2021-10-28T13:35:38.824241Z","iopub.status.idle":"2021-10-28T13:35:38.831678Z","shell.execute_reply.started":"2021-10-28T13:35:38.824199Z","shell.execute_reply":"2021-10-28T13:35:38.831023Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def compute_acc(predictions, target_labels):\n    return (np.array(predictions) == np.array(target_labels)).mean()\n# Precision = 모델이 infer한거 중에 맞춘 것\n# recall = 정답 중에 맞춘 것","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:50.239398Z","iopub.execute_input":"2021-10-28T13:35:50.240131Z","iopub.status.idle":"2021-10-28T13:35:50.244318Z","shell.execute_reply.started":"2021-10-28T13:35:50.240087Z","shell.execute_reply":"2021-10-28T13:35:50.243621Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# train_epoch = 15\ntrain_epoch = 10\nlowest_valid_loss = 9999.\nfor epoch in range(train_epoch):\n    with tqdm(train_loader, unit=\"batch\") as tepoch:\n        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n            tepoch.set_description(f\"Epoch {epoch}\")\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            token_type_ids = token_type_ids.to(device)\n            position_ids = position_ids.to(device)\n            labels = labels.to(device, dtype=torch.long)\n\n            optimizer.zero_grad()\n\n            output = model(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           token_type_ids=token_type_ids,\n                           position_ids=position_ids,\n                           labels=labels)\n\n            loss = output.loss\n            loss.backward()\n\n            optimizer.step()\n\n            tepoch.set_postfix(loss=loss.item())\n            if iteration != 0 and iteration % int(len(train_loader) / 5) == 0:\n                # Evaluate the model five times per epoch\n                with torch.no_grad():\n                    model.eval()\n                    valid_losses = []\n                    predictions = []\n                    target_labels = []\n                    for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n                                                                                                desc='Eval',\n                                                                                                position=1,\n                                                                                                leave=None):\n                        input_ids = input_ids.to(device)\n                        attention_mask = attention_mask.to(device)\n                        token_type_ids = token_type_ids.to(device)\n                        position_ids = position_ids.to(device)\n                        labels = labels.to(device, dtype=torch.long)\n\n                        output = model(input_ids=input_ids,\n                                       attention_mask=attention_mask,\n                                       token_type_ids=token_type_ids,\n                                       position_ids=position_ids,\n                                       labels=labels)\n\n                        logits = output.logits\n                        loss = output.loss\n                        valid_losses.append(loss.item())\n\n                        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n                        batch_labels = [int(example) for example in labels]\n\n                        predictions += batch_predictions\n                        target_labels += batch_labels\n\n                acc = compute_acc(predictions, target_labels)\n                valid_loss = sum(valid_losses) / len(valid_losses)\n                if lowest_valid_loss > valid_loss:\n                    print('Acc for model which have lower valid loss: ', acc)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T13:35:52.857331Z","iopub.execute_input":"2021-10-28T13:35:52.857819Z","iopub.status.idle":"2021-10-28T16:31:53.797003Z","shell.execute_reply.started":"2021-10-28T13:35:52.857781Z","shell.execute_reply":"2021-10-28T16:31:53.795265Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ntest_df = pd.read_csv('../input/goormtextclassificationproject/test_no_label.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:36:58.561897Z","iopub.execute_input":"2021-10-28T16:36:58.562758Z","iopub.status.idle":"2021-10-28T16:36:58.587361Z","shell.execute_reply.started":"2021-10-28T16:36:58.562705Z","shell.execute_reply":"2021-10-28T16:36:58.586684Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"test_dataset = test_df['Id']","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:37:05.500852Z","iopub.execute_input":"2021-10-28T16:37:05.501123Z","iopub.status.idle":"2021-10-28T16:37:05.519049Z","shell.execute_reply.started":"2021-10-28T16:37:05.501084Z","shell.execute_reply":"2021-10-28T16:37:05.516674Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def make_id_file_test(tokenizer, test_dataset):\n    data_strings = []\n    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]\n    for item in id_file_data:\n        data_strings.append(' '.join([str(k) for k in item]))\n    return data_strings","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:37:17.373376Z","iopub.execute_input":"2021-10-28T16:37:17.373909Z","iopub.status.idle":"2021-10-28T16:37:17.378226Z","shell.execute_reply.started":"2021-10-28T16:37:17.373870Z","shell.execute_reply":"2021-10-28T16:37:17.377601Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"test = make_id_file_test(tokenizer, test_dataset)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:37:20.499238Z","iopub.execute_input":"2021-10-28T16:37:20.499822Z","iopub.status.idle":"2021-10-28T16:37:20.887551Z","shell.execute_reply.started":"2021-10-28T16:37:20.499783Z","shell.execute_reply":"2021-10-28T16:37:20.886824Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test[:10]","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:37:21.931823Z","iopub.execute_input":"2021-10-28T16:37:21.932511Z","iopub.status.idle":"2021-10-28T16:37:21.938224Z","shell.execute_reply.started":"2021-10-28T16:37:21.932469Z","shell.execute_reply":"2021-10-28T16:37:21.937480Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class SentimentTestDataset(object):\n    def __init__(self, tokenizer, test):\n        self.tokenizer = tokenizer\n        self.data = []\n\n        for sent in test:\n            self.data += [self._cast_to_int(sent.strip().split())]\n\n    def _cast_to_int(self, sample):\n        return [int(word_id) for word_id in sample]\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        sample = self.data[index]\n        return np.array(sample)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:37:25.563589Z","iopub.execute_input":"2021-10-28T16:37:25.564327Z","iopub.status.idle":"2021-10-28T16:37:25.571312Z","shell.execute_reply.started":"2021-10-28T16:37:25.564289Z","shell.execute_reply":"2021-10-28T16:37:25.570625Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"test_dataset = SentimentTestDataset(tokenizer, test)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T16:37:28.204454Z","iopub.execute_input":"2021-10-28T16:37:28.205023Z","iopub.status.idle":"2021-10-28T16:37:28.212312Z","shell.execute_reply.started":"2021-10-28T16:37:28.204983Z","shell.execute_reply":"2021-10-28T16:37:28.211461Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# def collate_fn_sentiment_test(samples):\n#     input_ids = samples\n#     max_len = max(len(input_id) for input_id in input_ids)\n#     sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n\n#     input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n#                              batch_first=True)\n#     attention_mask = torch.tensor(\n#         [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n#          sorted_indices])\n#     token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n#     position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n\n#     return input_ids, attention_mask, token_type_ids, position_ids\n\ndef collate_fn_sentiment_test(samples):\n    input_ids = samples\n    max_len = max(len(input_id) for input_id in input_ids)\n    sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n\n    input_ids = pad_sequence([torch.tensor(input_id) for input_id in input_ids],\n                             batch_first=True)\n    attention_mask = torch.tensor(\n        [[1] * len(input_id) + [0] * (max_len - len(input_id)) for input_id in\n         input_ids])\n    token_type_ids = torch.tensor([[0] * len(input_id) for input_id in input_ids])\n    position_ids = torch.tensor([list(range(len(input_id))) for input_id in input_ids])\n\n    return input_ids, attention_mask, token_type_ids, position_ids","metadata":{"execution":{"iopub.status.busy":"2021-10-28T17:42:37.176904Z","iopub.execute_input":"2021-10-28T17:42:37.177205Z","iopub.status.idle":"2021-10-28T17:42:37.187445Z","shell.execute_reply.started":"2021-10-28T17:42:37.177170Z","shell.execute_reply":"2021-10-28T17:42:37.186715Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"test_batch_size = 32\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n                                          shuffle=False, collate_fn=collate_fn_sentiment_test,\n                                          num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T17:42:39.127116Z","iopub.execute_input":"2021-10-28T17:42:39.127868Z","iopub.status.idle":"2021-10-28T17:42:39.133372Z","shell.execute_reply.started":"2021-10-28T17:42:39.127817Z","shell.execute_reply":"2021-10-28T17:42:39.132715Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    model.eval()\n    predictions = []\n    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n                                                                        desc='Test',\n                                                                        position=1,\n                                                                        leave=None):\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        token_type_ids = token_type_ids.to(device)\n        position_ids = position_ids.to(device)\n\n        output = model(input_ids=input_ids,\n                       attention_mask=attention_mask,\n                       token_type_ids=token_type_ids,\n                       position_ids=position_ids)\n\n        logits = output.logits\n        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n        predictions += batch_predictions","metadata":{"execution":{"iopub.status.busy":"2021-10-28T17:42:40.364950Z","iopub.execute_input":"2021-10-28T17:42:40.365757Z","iopub.status.idle":"2021-10-28T17:42:41.496443Z","shell.execute_reply.started":"2021-10-28T17:42:40.365713Z","shell.execute_reply":"2021-10-28T17:42:41.495640Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"test_df['Category'] = predictions","metadata":{"execution":{"iopub.status.busy":"2021-10-28T17:42:42.806728Z","iopub.execute_input":"2021-10-28T17:42:42.807166Z","iopub.status.idle":"2021-10-28T17:42:42.814766Z","shell.execute_reply.started":"2021-10-28T17:42:42.807128Z","shell.execute_reply":"2021-10-28T17:42:42.813962Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"test_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-10-28T17:42:44.078100Z","iopub.execute_input":"2021-10-28T17:42:44.078660Z","iopub.status.idle":"2021-10-28T17:42:44.091373Z","shell.execute_reply.started":"2021-10-28T17:42:44.078622Z","shell.execute_reply":"2021-10-28T17:42:44.090498Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv('submission.csv')\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-28T17:42:45.873996Z","iopub.execute_input":"2021-10-28T17:42:45.874258Z","iopub.status.idle":"2021-10-28T17:42:45.892142Z","shell.execute_reply.started":"2021-10-28T17:42:45.874223Z","shell.execute_reply":"2021-10-28T17:42:45.891449Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}